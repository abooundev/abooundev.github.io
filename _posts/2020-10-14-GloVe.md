# GloVe

* Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. "Glove: Global vectors for word representation." *Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)*. 2014.
* 단어 임베딩 기법
* Word2Vec과 잠재 의미 분석 두 기법의 단점 극복
  * 잠재 의미 분석: 말뭉치 전체의 통계랑을 모두 활용할 수 있으나, 그 결과물로 단어 간 유사도 측정은 어려움
  * Word2Vec: 이 단어 벡터 사이의 유사도를 측정하는데 LSA보다 유리하지만, 사용자가 지정한 window내 local context만 학습하기 때문에 말뭉치 전체의 통계 정보는 반영되기 어려움
* **임베딩된 단어 벡터 간 유사도 측정**을 수월하게 하면서도 **말뭉치 전체의 통계 정보를 좀 더 잘 반영**해보자가 목표

cf. Levy&Goldberg(2014)

- GloVe 이후 발표

* Skip-gram 모델이 말뭉치 전체의 글로벌한 통계량인 SPMI행렬을 분해한 것과 동치라는 점을 증명



### 모델 구조

![image-20201013235845700](/Users/csg/Library/Application Support/typora-user-images/image-20201013235845700.png)

* 목적함수

  * 임베딩된 두 단어 벡터 내적이 말뭉치 전체 동시 등장 빈도의 로그값이 되도록 정의

  * 수식 함수

    $\mathcal{J} = \displaystyle\sum_{i,j=1}^{|V|}f(A_{i,j})(U_i\cdot V_j+b_i+b_j-logA_{i,j})^2$ 

    * 단어 i, j 각각 해당하는 벡터 $U_i, V_j$사이의 내적 값과 두 단어의 동시 빈도($A_i,j$)의 로그값 사이의 차이가 최소화될수록 학습 손실이 작아짐
    * bias항 ($b_i, b_j$)과 $f(A_i,j)$은 임베딩 품질을 높이기 위한 장치
    * $|V|$ : 어휘 집합 크기

* 학습 과정

  * 학습 말뭉치 대상으로 단어-문맥 행렬 $A$를 만듦
    * (예) $|V|$ =10,000-> 행렬 크기=10,000x10,000
  * 목적함수를 최소화하는 임베딩 벡터를 찾기 위한 행렬 분해(matrix factorization) 수행
    * 행렬 $U,V$를 랜덤으로 초기화 
    * 목적함수를 최소화하는 방향으로 $U,V$를 조금씩 업데이트
    * 학습 손실이 더 줄지 않거나 정해진 step수 만큼 학습했을 경우 학습 종료
    * 학습이 끝난 $U$를 단어 임베딩으로 사용
    * $U+V^T, U와V^T$를 이어붙여(concatenation) 임베딩으로 사용 가능
