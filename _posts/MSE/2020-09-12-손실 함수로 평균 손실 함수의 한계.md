---
title: "손실 함수로 평균 손실 함수의 한계"
date: 2020-09-12
math: true

---


# 평균 제곱 오차 (MSE, Mean Square Error)

$$e = \frac{1}{2}||y-o||^2_2$$

$e$: 오류

$o$: 신경망 출력 

$y$: 샘플의 레이블, 목푯값

* 오류$(e)$는 신경망 출력$(o)$이 레이블$(y)$과 같을때, 0이 되고 둘의 차이가 클 수록 커짐

* 한계

  * 오류가 더 큰 경우에 gradient 값이 더 작아져, 학습이 느리게 이루어짐

    * 방향은 제대로 알려주지만, 이동하는 양이 gradient에 비례하므로 학습이 느려짐

  * (예시) 

    * 입력 노드 1개, 출력 노드 1개인 신경망
      * **activation function: logistic sigmoid**
      * objective function: MSE
        * $e = \frac{1}{2}(y-o)^2 = \frac{1}{2}(y - \sigma(wx+b))^2$
      * 1번 신경망: $w=0.4, b=0.5$
      * 2번 신경망: $w=1.9, b=3.0$
    * Sample: $ x=1.5, y = 0.0$

    ![](https://drive.google.com/uc?export=view&id=1O3KaIG3EJHLT0vD8N4F67OrV79IIFuE6)


|          | 1번           | 2번           | 설명                   |
| -------- | ------------- | ------------- | ---------------------- |
| error    | 0.2815        | 0.4971        | 2번이 더 큰 오류임     |
| gradient | 0.2109/0.1406 | 0.0043/0.0029 | 2번 gradient가 더 작음 |


  * 오류가 너무 크거나 작을 때,  gradient가 매우 작아지는 이유?

    * logistic sigmoid 함수의 도함수는 입력값이 0일 때 가장 크고, 입력값이 크거나 작아지면 0으로 수렴함

    * 가중치 $w$의 gradient: $\frac{\partial e}{\partial w}=-(y-o)x\sigma '(wx+b)$

    * 바이어스 $b$의 gradient:$\frac{\partial e}{\partial b}=-(y-o)\sigma '(wx+b)$

    * 도함수 $\sigma '(wx+b)$에서 입력값 $wx+b$가 커지면 0으로 수렴해 gradient가 매우 작아짐 

      ![](https://drive.google.com/uc?export=view&id=1mgHB6mOj8NL3tdL7GjSO_KLCmhUX_rwg)
